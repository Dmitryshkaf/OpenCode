{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка модели YOLOv8\n",
    "model = YOLO('yolov9c.pt')\n",
    "\n",
    "# Список цветов для различных классов\n",
    "colors = [\n",
    "    (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255),\n",
    "    (255, 0, 255), (192, 192, 192), (128, 128, 128), (128, 0, 0), (128, 128, 0),\n",
    "    (0, 128, 0), (128, 0, 128), (0, 128, 128), (0, 0, 128), (72, 61, 139),\n",
    "    (47, 79, 79), (47, 79, 47), (0, 206, 209), (148, 0, 211), (255, 20, 147)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK№1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для обработки изображения\n",
    "def process_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    results = model(image)[0]\n",
    "    \n",
    "    # Получение оригинального изображения и результатов\n",
    "    image = results.orig_img\n",
    "    classes_names = results.names\n",
    "    classes = results.boxes.cls.cpu().numpy()\n",
    "    boxes = results.boxes.xyxy.cpu().numpy().astype(np.int32)\n",
    "\n",
    "    grouped_objects = {}\n",
    "\n",
    "    # Рисование рамок и группировка результатов\n",
    "    for class_id, box in zip(classes, boxes):\n",
    "        if class_id==0:    \n",
    "            class_name = classes_names[int(class_id)]\n",
    "            color = colors[int(class_id) % len(colors)]  \n",
    "            if class_name not in grouped_objects:\n",
    "                grouped_objects[class_name] = []\n",
    "                \n",
    "            x1, y1, x2, y2 = box\n",
    "            center_x = (x1 + x2) // 2\n",
    "            center_y = (y1 + y2) // 2\n",
    "            grouped_objects[class_name].append((box, (center_x, center_y)))\n",
    "            \n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(image, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "            \n",
    "    count_of_persons = len([el for el in classes if el == 0])\n",
    "     \n",
    "    cv2.putText(image, f\"Count of persons: {count_of_persons}\", (450,370), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Сохранение измененного изображения\n",
    "    new_image_path = os.path.splitext(image_path)[0] + '_yolo' + os.path.splitext(image_path)[1]\n",
    "    cv2.imwrite(new_image_path, image)\n",
    "\n",
    "    # Сохранение данных в текстовый файл\n",
    "    text_file_path = os.path.splitext(image_path)[0] + '_data.txt'\n",
    "    with open(text_file_path, 'w') as f:\n",
    "        for class_name, details in grouped_objects.items():\n",
    "            f.write(f\"{class_name}:\\n\")\n",
    "            for box, center in details:\n",
    "                f.write(f\"Coordinates: ({box[0]}, {box[1]}, {box[2]}, {box[3]}) Center: ({center[0]}, {center[1]})\\n\")\n",
    "    \n",
    "   \n",
    "    \n",
    "    print(f\"Count of persons: {count_of_persons}\")\n",
    "    print(f\"Processed {image_path}:\")\n",
    "    print(f\"Saved bounding-box image to {new_image_path}\")\n",
    "    print(f\"Saved data to {text_file_path}\")\n",
    "    return results, grouped_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 6 persons, 1 motorcycle, 1 bench, 236.0ms\n",
      "Speed: 5.0ms preprocess, 236.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Count of persons: 6\n",
      "Processed data/test_imgs/720x.png:\n",
      "Saved bounding-box image to data/test_imgs/720x_yolo.png\n",
      "Saved data to data/test_imgs/720x_data.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res, objects = process_image('data/test_imgs/720x.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[146, 192, 181, 272, 214, 251, 232, 298, 258, 298]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[146, 154, 298, 298]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Получение центров баундбоксов людей и построение групп\n",
    "centers = np.array([objects['person'][el][1]  for el in range(len(objects['person']))])\n",
    "\n",
    "groups_of_persons= {}\n",
    "\n",
    "num_rectangles = centers.shape[0]\n",
    "distance_matrix = np.zeros((num_rectangles, num_rectangles))\n",
    "counter=-1\n",
    "for i in range(num_rectangles):\n",
    "    for j in range(num_rectangles):\n",
    "        if i != j:\n",
    "            distance_matrix[i, j] = np.linalg.norm(centers[i] - centers[j])\n",
    "            if distance_matrix[i,j]<100:\n",
    "                all_values = [item for sublist in groups_of_persons.values() for item in sublist]\n",
    "                if i not in all_values  and j not in all_values:\n",
    "                    counter+=1\n",
    "                    groups_of_persons.update({f\"Group №{counter}\" :[i,  j]})\n",
    "                else:\n",
    "                    groups_of_persons[f\"Group №{counter}\"].extend([i, j])\n",
    "#Получение людей в группах          \n",
    "unique_list = []\n",
    "for group in groups_of_persons.keys(): \n",
    "    persons_in_group = np.array(groups_of_persons[group])\n",
    "    unique_persons = np.unique(persons_in_group)\n",
    "    unique_list.append(unique_persons)\n",
    "# Получение координат баундбоксов\n",
    "list_of_coordinates=[]\n",
    "for group in range(len(unique_list)): \n",
    "    list_of_x=[]\n",
    "    list_of_y=[]\n",
    "    for num in unique_list[group]:\n",
    "        list_of_x.extend([objects[\"person\"][num][0][0], objects[\"person\"][num][0][2]])\n",
    "        list_of_y.extend([objects[\"person\"][num][0][1], objects[\"person\"][num][0][3]])\n",
    "    list_of_coordinates.append([list_of_x,list_of_y])\n",
    "        \n",
    "boundnoxes = []\n",
    "for coordinates in list_of_coordinates:\n",
    "    xmin, xmax = min(coordinates[0]), max(coordinates[0])\n",
    "    print(coordinates[0])\n",
    "    ymin, ymax = min(coordinates[1]), max(coordinates[1])\n",
    "    boundnoxes.append([xmin,ymin,xmax,ymax])\n",
    "boundnoxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK№2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawing_boundboxes(image_path, boundnoxes, unique_list):\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    class_name = \"Group\"\n",
    "    color = colors[int(2) % len(colors)] \n",
    "    grouped_objects = {}\n",
    "    \n",
    "    for box in boundnoxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        \n",
    "        grouped_objects[class_name] = []\n",
    "        grouped_objects[class_name].append(box)\n",
    "        \n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(image, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "     \n",
    "    count_of_groups = len(boundnoxes)\n",
    "    count_of_persons = [len(unic) for unic in unique_list]\n",
    "    \n",
    "    cv2.putText(image, f\"Count of Groups: {count_of_groups}\", (450,320), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    cv2.putText(image, f\"Count of People in Group: {count_of_persons}\", (450,350), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    new_image_path = os.path.splitext(image_path)[0] + '_grouped' + os.path.splitext(image_path)[1]\n",
    "    cv2.imwrite(new_image_path, image)\n",
    "\n",
    "    \n",
    "    text_file_path = os.path.splitext(image_path)[0] + '_data2.txt'\n",
    "    with open(text_file_path, 'w') as f:\n",
    "        for class_name, details in grouped_objects.items():\n",
    "            f.write(f\"{class_name}:\\n\")\n",
    "            for box in details:\n",
    "                f.write(f\"Coordinates: ({box[0]}, {box[1]}, {box[2]}, {box[3]})\\n\")\n",
    "\n",
    "   \n",
    "    \n",
    "    print(f\"Count of groups: {count_of_groups}\")\n",
    "    print(f\"Processed {image_path}:\")\n",
    "    print(f\"Saved bounding-box image to {new_image_path}\")\n",
    "    print(f\"Saved data to {text_file_path}\")\n",
    "    return grouped_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of groups: 1\n",
      "Processed data/test_imgs/720x_yolo.png:\n",
      "Saved bounding-box image to data/test_imgs/720x_yolo_grouped.png\n",
      "Saved data to data/test_imgs/720x_yolo_data2.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grouped_res = drawing_boundboxes('data/test_imgs/720x_yolo.png', boundnoxes, unique_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK№3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model for helmets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.48 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.45  Python-3.12.3 torch-2.2.2 CUDA:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=c:\\Developer\\\\OpenCode\\OpenCode\\yolov9c.pt, data=c:\\Developer\\\\OpenCode\\OpenCode\\data.yaml, epochs=250, time=None, patience=100, batch=10, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=red28, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=C:\\Users\\pendo\\runs\\detect\\red28\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    212864  ultralytics.nn.modules.block.RepNCSPELAN4    [128, 256, 128, 64, 1]        \n",
      "  3                  -1  1    164352  ultralytics.nn.modules.block.ADown           [256, 256]                    \n",
      "  4                  -1  1    847616  ultralytics.nn.modules.block.RepNCSPELAN4    [256, 512, 256, 128, 1]       \n",
      "  5                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      "  6                  -1  1   2857472  ultralytics.nn.modules.block.RepNCSPELAN4    [512, 512, 512, 256, 1]       \n",
      "  7                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      "  8                  -1  1   2857472  ultralytics.nn.modules.block.RepNCSPELAN4    [512, 512, 512, 256, 1]       \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPELAN         [512, 512, 256]               \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1   3119616  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 512, 512, 256, 1]      \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    912640  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 256, 256, 128, 1]      \n",
      " 16                  -1  1    164352  ultralytics.nn.modules.block.ADown           [256, 256]                    \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1   2988544  ultralytics.nn.modules.block.RepNCSPELAN4    [768, 512, 512, 256, 1]       \n",
      " 19                  -1  1    656384  ultralytics.nn.modules.block.ADown           [512, 512]                    \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   3119616  ultralytics.nn.modules.block.RepNCSPELAN4    [1024, 512, 512, 256, 1]      \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m640\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 11\u001b[0m     results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrain(data \u001b[38;5;241m=\u001b[39m data_path,\n\u001b[0;32m     12\u001b[0m                           epochs \u001b[38;5;241m=\u001b[39m epochs,\n\u001b[0;32m     13\u001b[0m                           batch \u001b[38;5;241m=\u001b[39m batch,\n\u001b[0;32m     14\u001b[0m                           imgsz \u001b[38;5;241m=\u001b[39m image_size,\n\u001b[0;32m     15\u001b[0m                           name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m                           device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pendo\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\ultralytics\\engine\\model.py:646\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m (trainer \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m\"\u001b[39m))(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[1;32m--> 646\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pendo\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\train.py:88\u001b[0m, in \u001b[0;36mDetectionTrainer.get_model\u001b[1;34m(self, cfg, weights, verbose)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a YOLO detection model.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     model \u001b[38;5;241m=\u001b[39m DetectionModel(cfg, nc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnc\u001b[39m\u001b[38;5;124m\"\u001b[39m], verbose\u001b[38;5;241m=\u001b[39mverbose \u001b[38;5;129;01mand\u001b[39;00m RANK \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weights:\n\u001b[0;32m     90\u001b[0m         model\u001b[38;5;241m.\u001b[39mload(weights)\n",
      "File \u001b[1;32mc:\\Users\\pendo\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:309\u001b[0m, in \u001b[0;36mDetectionModel.__init__\u001b[1;34m(self, cfg, ch, nc, verbose)\u001b[0m\n\u001b[0;32m    307\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverriding model.yaml nc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with nc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m nc  \u001b[38;5;66;03m# override YAML value\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;241m=\u001b[39m parse_model(deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml), ch\u001b[38;5;241m=\u001b[39mch, verbose\u001b[38;5;241m=\u001b[39mverbose)  \u001b[38;5;66;03m# model, savelist\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m {i: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnc\u001b[39m\u001b[38;5;124m\"\u001b[39m])}  \u001b[38;5;66;03m# default names dict\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaml\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minplace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\pendo\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:985\u001b[0m, in \u001b[0;36mparse_model\u001b[1;34m(d, ch, verbose)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    983\u001b[0m     c2 \u001b[38;5;241m=\u001b[39m ch[f]\n\u001b[1;32m--> 985\u001b[0m m_ \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m(m(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n))) \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m m(\u001b[38;5;241m*\u001b[39margs)  \u001b[38;5;66;03m# module\u001b[39;00m\n\u001b[0;32m    986\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(m)[\u001b[38;5;241m8\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# module type\u001b[39;00m\n\u001b[0;32m    987\u001b[0m m\u001b[38;5;241m.\u001b[39mnp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(x\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m m_\u001b[38;5;241m.\u001b[39mparameters())  \u001b[38;5;66;03m# number params\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pendo\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:45\u001b[0m, in \u001b[0;36mDetect.__init__\u001b[1;34m(self, nc, ch)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m     42\u001b[0m     nn\u001b[38;5;241m.\u001b[39mSequential(Conv(x, c2, \u001b[38;5;241m3\u001b[39m), Conv(c2, c2, \u001b[38;5;241m3\u001b[39m), nn\u001b[38;5;241m.\u001b[39mConv2d(c2, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_max, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ch\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(nn\u001b[38;5;241m.\u001b[39mSequential(Conv(x, c3, \u001b[38;5;241m3\u001b[39m), Conv(c3, c3, \u001b[38;5;241m3\u001b[39m), nn\u001b[38;5;241m.\u001b[39mConv2d(c3, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnc, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ch)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfl \u001b[38;5;241m=\u001b[39m DFL(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_max) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_max \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend2end:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone2one_cv2 \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2)\n",
      "File \u001b[1;32mc:\\Users\\pendo\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:65\u001b[0m, in \u001b[0;36mDFL.__init__\u001b[1;34m(self, c1)\u001b[0m\n\u001b[0;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(c1, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata[:] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, c1, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc1 \u001b[38;5;241m=\u001b[39m c1\n",
      "File \u001b[1;32mc:\\Users\\pendo\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1690\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1687\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, value: Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_from\u001b[39m(\u001b[38;5;241m*\u001b[39mdicts_or_sets):\n\u001b[0;32m   1692\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dicts_or_sets:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()  \n",
    "\n",
    "data_path = os.path.join(current_dir, 'data.yaml')\n",
    "model = YOLO(os.path.join(current_dir, 'yolov9c.pt'))\n",
    "\n",
    "epochs = 250\n",
    "batch = 10\n",
    "image_size = 640\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    results = model.train(data = data_path,\n",
    "                          epochs = epochs,\n",
    "                          batch = batch,\n",
    "                          imgsz = image_size,\n",
    "                          name = 'red',\n",
    "                          device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = YOLO('detect/final/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_inside(box1, box2):\n",
    "    \"\"\"\n",
    "    проверяет, находится ли box2!!! внутри box1!!!\n",
    "    \"\"\"\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2\n",
    "\n",
    "    return (x1_1 <= x1_2 <= x2_1 and y1_1 <= y1_2 <= y2_1) or (x1_1 <= x2_2 <= x2_1 and y1_1 <= y2_2 <= y2_1)\n",
    "    \n",
    "\n",
    "def process_with_helmets(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    results = model(image)[0]\n",
    "    \n",
    "    image = results.orig_img\n",
    "    classes_names = results.names\n",
    "    classes = results.boxes.cls.cpu().numpy()\n",
    "    boxes = results.boxes.xyxy.cpu().numpy().astype(np.int32)\n",
    "    \n",
    "    helmets_results = model1(image)[0]\n",
    "    image = helmets_results.orig_img\n",
    "    helmet_classes_names = helmets_results.names\n",
    "    helmet_classes = helmets_results.boxes.cls.cpu().numpy()\n",
    "    helmet_boxes = helmets_results.boxes.xyxy.cpu().numpy().astype(np.int32)\n",
    "\n",
    "    grouped_objects = {}\n",
    "    unique_helmet_boxes = set()\n",
    "    count_of_helmets =0\n",
    "    \n",
    "    for class_id, box in zip(classes, boxes):\n",
    "        if class_id == 0:\n",
    "            class_name = classes_names[int(class_id)]\n",
    "            color = colors[int(class_id) % len(colors)]\n",
    "            if class_name not in grouped_objects:\n",
    "                grouped_objects[class_name] = []\n",
    "                \n",
    "            x1, y1, x2, y2 = box\n",
    "            center_x = (x1 + x2) // 2\n",
    "            center_y = (y1 + y2) // 2\n",
    "            grouped_objects[class_name].append((box, (center_x, center_y)))\n",
    "            \n",
    "            for h_class_id, h_box in zip(helmet_classes, helmet_boxes):\n",
    "                if h_class_id == 0:\n",
    "                    h_class_name = helmet_classes_names[int(h_class_id)]\n",
    "                    h_color = colors[int(h_class_id) % len(colors)+1]\n",
    "                    if is_inside(box, h_box):\n",
    "                        h_box_tuple = tuple(h_box)\n",
    "                        if h_box_tuple not in unique_helmet_boxes:\n",
    "                            if h_class_name not in grouped_objects:\n",
    "                                grouped_objects[h_class_name] = []\n",
    "                            h_x1, h_y1, h_x2, h_y2 = h_box\n",
    "                            center_h_x = (h_x1 + h_x2) // 2\n",
    "                            center_h_y = (h_y1 + h_y2) // 2\n",
    "                            grouped_objects[h_class_name].append((h_box, (center_h_x, center_h_y)))\n",
    "\n",
    "                            count_of_helmets += 1\n",
    "                            unique_helmet_boxes.add(h_box_tuple)\n",
    "\n",
    "                            cv2.rectangle(image, (h_x1, h_y1), (h_x2, h_y2), h_color, 2)\n",
    "                            cv2.putText(image, h_class_name, (h_x1, h_y1 - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.4, h_color, 1)\n",
    "            \n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(image, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "            \n",
    "    count_of_persons = len([el for el in classes if el == 0])\n",
    "     \n",
    "    cv2.putText(image, f\"Count of persons: {count_of_persons}\", (440, 365), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    cv2.putText(image, f\"Count of people in helmets: {count_of_helmets}\", (440, 380), cv2.FONT_HERSHEY_SIMPLEX, 0.5, h_color, 2)\n",
    "    cv2.putText(image, f\"Count of people without helmets: {count_of_persons - count_of_helmets}\", (435, 395), cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[2], 2)\n",
    "\n",
    "    new_image_path = os.path.splitext(image_path)[0] + '_yolo_helmet' + os.path.splitext(image_path)[1]\n",
    "    cv2.imwrite(new_image_path, image)\n",
    "\n",
    "    print(grouped_objects)\n",
    "\n",
    "    text_file_path = os.path.splitext(image_path)[0] + '_data_helmet.txt'\n",
    "    with open(text_file_path, 'w') as f:\n",
    "        for class_name, details in grouped_objects.items():\n",
    "            f.write(f\"{class_name}:\\n\")\n",
    "            for box, center in details:\n",
    "                f.write(f\"Coordinates: ({box[0]}, {box[1]}, {box[2]}, {box[3]}) Center: ({center[0]}, {center[1]})\\n\")\n",
    "    \n",
    "    print(f\"Count of persons: {count_of_persons}\")\n",
    "    print(f\"Processed {image_path}:\")\n",
    "    print(f\"Saved bounding-box image to {new_image_path}\")\n",
    "    print(f\"Saved data to {text_file_path}\")\n",
    "    return results, grouped_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 6 persons, 1 motorcycle, 1 bench, 239.0ms\n",
      "Speed: 5.0ms preprocess, 239.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 helmets, 22.0ms\n",
      "Speed: 4.0ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'person': [(array([146, 173, 192, 298]), (169, 235)), (array([181, 212, 272, 286]), (226, 249)), (array([637,  14, 656,  70]), (646, 42)), (array([214, 170, 251, 226]), (232, 198)), (array([232, 154, 298, 224]), (265, 189)), (array([258, 154, 298, 222]), (278, 188))], 'helmet': [(array([169, 173, 193, 197]), (181, 185)), (array([225, 211, 252, 235]), (238, 223)), (array([269, 153, 294, 177]), (281, 165)), (array([244, 156, 264, 174]), (254, 165))]}\n",
      "Count of persons: 6\n",
      "Processed data/test_imgs/720x.png:\n",
      "Saved bounding-box image to data/test_imgs/720x_yolo_helmet.png\n",
      "Saved data to data/test_imgs/720x_data_helmet.txt\n"
     ]
    }
   ],
   "source": [
    "res3, objects3 = process_with_helmets('data/test_imgs/720x.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "<Pytorch- GPU>",
   "language": "python",
   "name": "pytorch-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
